{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9af0b0b9-2c3c-4d30-b7c6-997c58bc60aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T09:12:50.372899Z",
     "iopub.status.busy": "2025-11-17T09:12:50.372453Z",
     "iopub.status.idle": "2025-11-17T09:13:05.618752Z",
     "shell.execute_reply": "2025-11-17T09:13:05.617463Z",
     "shell.execute_reply.started": "2025-11-17T09:12:50.372859Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 09:12:53,055\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-11-17 09:12:53,643\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-11-17 09:12:54,397\tINFO client_builder.py:241 -- Passing the following kwargs to ray.init() on the server: log_to_driver\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [MLOps Platform SDK] ---\n",
      "Successfully connected to Ray at ray://ray-head:10001\n",
      "MLflow Tracking Server configured for: http://mlflow-server:5000\n",
      "--- SDK Ready ---\n"
     ]
    }
   ],
   "source": [
    "import my_sdk\n",
    "\n",
    "my_sdk.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eff3c791-a273-4f0a-abbc-ec3911c8df44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_internal_explorative_take_batch pid=718)\u001b[0m Estimated parquet encoding ratio is 6.463.\n",
      "\u001b[36m(_internal_explorative_take_batch pid=718)\u001b[0m Estimated parquet reader batch size at 163881 rows\n",
      "\u001b[36m(_internal_explorative_take_batch pid=718)\u001b[0m A new progress UI is available. To enable, set `ray.data.DataContext.get_current().enable_rich_progress_bars = True`.\n",
      "\u001b[36m(_internal_explorative_take_batch pid=718)\u001b[0m Registered dataset logger for dataset dataset_5_0\n",
      "\u001b[36m(_internal_explorative_take_batch pid=718)\u001b[0m Starting execution of Dataset dataset_5_0. Full logs are in /tmp/ray/session_2025-11-17_00-59-52_408689_7/logs/ray-data\n",
      "\u001b[36m(_internal_explorative_take_batch pid=718)\u001b[0m Execution plan of Dataset dataset_5_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> LimitOperator[limit=64]\n",
      "\u001b[36m(_internal_explorative_take_batch pid=718)\u001b[0m ⚠️  Ray's object store is configured to use only 42.9% of available memory (11.2GiB out of 26.1GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
      "\u001b[36m(_internal_explorative_take_batch pid=718)\u001b[0m ✔️  Dataset dataset_5_0 execution finished in 7.32 seconds\n"
     ]
    }
   ],
   "source": [
    "batch = my_sdk.explorative_take_batch(\"s3://mybucket/data-delta\")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90ae3849-8ce2-4a2c-b2da-d85dc836bc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [SDK] No active run to end.\n",
      "--- [SDK] Finding notebook path to set experiment...\n",
      "--- [SDK] Automatically setting experiment to: 'ali.ipynb' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 09:00:40 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial message can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
      "    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
      "    - error|e|exception|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [SDK] Starting run: RRsdsRun (ID: e78e094a51104f1499b3726d6ba8acbe) ---\n",
      "Attempting to register model TT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 09:00:40 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected scikit-learn model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/11/17 09:00:43 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'TT' already exists. Creating a new version of this model...\n",
      "2025/11/17 09:00:43 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: TT, version 2\n",
      "Created version '2' of model 'TT'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# --- Train a scikit-learn model ---\n",
    "X, y = load_iris(return_X_y=True)\n",
    "sklearn_model = RandomForestClassifier().fit(X, y)\n",
    "\n",
    "my_custom_tags = {\n",
    "    \"tag\": 0.98,\n",
    "    \"model_type\": \"RandomForest\"\n",
    "}\n",
    "my_sdk.end_run()\n",
    "my_sdk.start_run(\"MTMRUN\")\n",
    "my_sdk.register_model(sklearn_model, \"MTM\", tags=my_custom_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f99e2d-9776-47b6-85fa-173027473061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_internal_explorative_take_batch pid=717)\u001b[0m Estimated parquet encoding ratio is 6.463.\n",
      "\u001b[36m(_internal_explorative_take_batch pid=717)\u001b[0m Estimated parquet reader batch size at 163881 rows\n",
      "\u001b[36m(_internal_explorative_take_batch pid=717)\u001b[0m A new progress UI is available. To enable, set `ray.data.DataContext.get_current().enable_rich_progress_bars = True`.\n",
      "\u001b[36m(_internal_explorative_take_batch pid=717)\u001b[0m Registered dataset logger for dataset dataset_3_0\n",
      "\u001b[36m(_internal_explorative_take_batch pid=717)\u001b[0m Starting execution of Dataset dataset_3_0. Full logs are in /tmp/ray/session_2025-11-17_00-59-52_408689_7/logs/ray-data\n",
      "\u001b[36m(_internal_explorative_take_batch pid=717)\u001b[0m Execution plan of Dataset dataset_3_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> LimitOperator[limit=2]\n",
      "\u001b[36m(_internal_explorative_take_batch pid=717)\u001b[0m ⚠️  Ray's object store is configured to use only 42.9% of available memory (11.2GiB out of 26.1GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
      "\u001b[36m(_internal_explorative_take_batch pid=717)\u001b[0m ✔️  Dataset dataset_3_0 execution finished in 4.46 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClientObjectRef(32d950ec0ccf9d2a76d955ebd56cee39372e71090100000001000000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = my_sdk.explorative_take_batch(\"s3://mybucket/data-delta/\", 2, batch_format = \"pyarrow\")\n",
    "writer = my_sdk.initiate_data_writer(\"s3://testbucket/\")\n",
    "my_sdk.write_data(writer, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae33641b-8d6b-4c4a-8a73-3374a7135044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_sdk\n",
    "config = my_sdk.TrainConfig(\n",
    "    model_name=\"NAME\",\n",
    "    delta_path=\"s3://mybucket/data-delta/\",\n",
    "    epochs=2,\n",
    "    batch_size=50,\n",
    "    num_workers=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "875d808a-eee0-4e92-8be3-5f45fe675064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [MLOps Platform SDK] ---\n",
      "Ray is already connected.\n",
      "MLflow Tracking Server configured for: http://mlflow-server:5000\n",
      "--- SDK Ready ---\n"
     ]
    }
   ],
   "source": [
    "import my_sdk\n",
    "\n",
    "my_sdk.init()\n",
    "\n",
    "config = my_sdk.TrainConfig(\n",
    "    model_name=\"NAME\",\n",
    "    delta_path=\"s3://mybucket/data-delta/\",\n",
    "    epochs=2,\n",
    "    batch_size=50,\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "class MyTrainer(my_sdk.BaseTrainWorker):\n",
    "    def init_model(self):\n",
    "        return {\"w\": 0.0}\n",
    "\n",
    "    def initialize(self):\n",
    "        print(\"[User] Init step\")\n",
    "\n",
    "    def train_step(self, model, batch, epoch, config):\n",
    "        x = batch[\"x\"]\n",
    "        model[\"w\"] += sum(x) * 0.01\n",
    "        return {\"update\": float(model[\"w\"])}\n",
    "\n",
    "    def post_epoch(self, epoch, metrics):\n",
    "        print(f\"[User] Post epoch {epoch}, metrics_count={len(metrics)}\")\n",
    "\n",
    "    def post_train(self, final_metrics):\n",
    "        print(\"[User] Training complete.\")\n",
    "\n",
    "result = my_sdk.train(MyTrainer, config)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c1d6892-c43c-48af-9114-fd6e6b29d639",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = my_sdk.train(MyTrainer, config)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8e9bc00-1b1b-42e4-862f-0177406febed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_sdk.load_model(\"MTM\", version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f3d3b3e-cfee-4f91-929e-91d46bd7d4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing project files...\n",
      "['subfolder-2', 'mohamadsNotebook.ipynb', 'example1.dash', 'test.ipynb', 'yhg.ipynb', 'renderer.ipynb', 'salam.ipynb', 'README.md', 'train_test.ipynb', 'ali.ipynb', 'ipywidgets.ipynb', 'newModel.ipynb', 'xbox.ipynb', 'tmp.ipynb', 'example1.ipynb', 'panel.ipynb', 'collaboration.ipynb', 'MLflow_Tracking_URI', 'somethingggggg (Copy).ipynb', 'someOtherThingsOrNameOrSomethingElseOrChiz.ipynb', 'subfolder-1', 'ping.ipynb', 'matplotlib.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import my_sdk\n",
    "\n",
    "# And access it this way:\n",
    "my_files = my_sdk.get_project_files()\n",
    "print(my_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3da9822c-3a63-4568-9a8c-6163f436e539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainWorkerActor pid=273, ip=172.19.0.4)\u001b[0m [Worker] Running setup...\n",
      "[Driver] Starting epoch 0\n",
      "\u001b[36m(TrainWorkerActor pid=273, ip=172.19.0.4)\u001b[0m [User] Init step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainWorkerActor pid=273, ip=172.19.0.4)\u001b[0m Estimated parquet encoding ratio is 6.463.\n",
      "\u001b[36m(TrainWorkerActor pid=273, ip=172.19.0.4)\u001b[0m Estimated parquet reader batch size at 163881 rows\n",
      "\u001b[36m(TrainWorkerActor pid=273, ip=172.19.0.4)\u001b[0m A new progress UI is available. To enable, set `ray.data.DataContext.get_current().enable_rich_progress_bars = True`.\n",
      "\u001b[36m(TrainWorkerActor pid=273, ip=172.19.0.4)\u001b[0m Registered dataset logger for dataset dataset_13_0\n",
      "\u001b[36m(TrainWorkerActor pid=273, ip=172.19.0.4)\u001b[0m Starting execution of Dataset dataset_13_0. Full logs are in /tmp/ray/session_2025-11-17_00-59-52_408689_7/logs/ray-data\n",
      "\u001b[36m(TrainWorkerActor pid=273, ip=172.19.0.4)\u001b[0m Execution plan of Dataset dataset_13_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> AllToAllOperator[RandomShuffle]\n",
      "\u001b[36m(TrainWorkerActor pid=273, ip=172.19.0.4)\u001b[0m ⚠️  Ray's object store is configured to use only 42.9% of available memory (11.2GiB out of 26.1GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
      "\u001b[33m(raylet, ip=172.19.0.4)\u001b[0m [2025-11-17 01:08:07,909 W 435 435] rpc_client.h:153: Failed to connect to GCS at address ray-head:6379 within 5 seconds.\n",
      "\u001b[36m(TrainWorkerActor pid=273, ip=172.19.0.4)\u001b[0m ✔️  Dataset dataset_13_0 execution finished in 134.49 seconds\n"
     ]
    },
    {
     "ename": "RayTaskError(TypeError)",
     "evalue": "\u001b[36mray::TrainWorkerActor.train_epoch()\u001b[39m (pid=273, ip=172.19.0.4, actor_id=012b9ff22cdea402925c322f01000000, repr=<__main__.TrainWorkerActor object at 0x7dae17b67b90>)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_20/1999435513.py\", line 92, in train_epoch\nTypeError: not all arguments converted during string formatting",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRayTaskError(TypeError)\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 179\u001b[39m\n\u001b[32m    169\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[User] Training complete.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    172\u001b[39m config = TrainConfig(\n\u001b[32m    173\u001b[39m     delta_path=\u001b[33m\"\u001b[39m\u001b[33ms3://mybucket/data-delta/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    174\u001b[39m     epochs=\u001b[32m2\u001b[39m,\n\u001b[32m    175\u001b[39m     batch_size=\u001b[32m50\u001b[39m,\n\u001b[32m    176\u001b[39m     num_workers=\u001b[32m1\u001b[39m\n\u001b[32m    177\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m result =  \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMyTrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(trainer_cls, config)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(trainer_cls, config: TrainConfig):\n\u001b[32m    105\u001b[39m     trainer = BaseTrainer(config=config, trainer_cls=trainer_cls)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 130\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Driver] Starting epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    129\u001b[39m     futs = [w.train_epoch.remote(epoch) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m workers]\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     epoch_results = \u001b[43mray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m     all_epochs_results.append(epoch_results)\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# finalize\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:22\u001b[39m, in \u001b[36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mauto_init_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     21\u001b[39m     auto_init_ray()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py:103\u001b[39m, in \u001b[36mclient_mode_hook.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m client_mode_should_convert():\n\u001b[32m    100\u001b[39m     \u001b[38;5;66;03m# Legacy code\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# we only convert init function if RAY_CLIENT_MODE=1\u001b[39;00m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func.\u001b[34m__name__\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33minit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/ray/util/client/api.py:42\u001b[39m, in \u001b[36m_ClientAPI.get\u001b[39m\u001b[34m(self, vals, timeout)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, vals, *, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     36\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"get is the hook stub passed on to replace `ray.get`\u001b[39;00m\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03m        vals: [Client]ObjectRef or list of these refs to retrieve.\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m        timeout: Optional timeout in milliseconds\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/ray/util/client/worker.py:433\u001b[39m, in \u001b[36mWorker.get\u001b[39m\u001b[34m(self, vals, timeout)\u001b[39m\n\u001b[32m    431\u001b[39m     op_timeout = max_blocking_operation_time\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_get\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GetTimeoutError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/ray/util/client/worker.py:461\u001b[39m, in \u001b[36mWorker._get\u001b[39m\u001b[34m(self, ref, timeout)\u001b[39m\n\u001b[32m    459\u001b[39m         logger.exception(\u001b[33m\"\u001b[39m\u001b[33mFailed to deserialize \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(chunk.error))\n\u001b[32m    460\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunk.total_size > OBJECT_TRANSFER_WARNING_SIZE \u001b[38;5;129;01mand\u001b[39;00m log_once(\n\u001b[32m    463\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mclient_object_transfer_size_warning\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    464\u001b[39m ):\n\u001b[32m    465\u001b[39m     size_gb = chunk.total_size / \u001b[32m2\u001b[39m**\u001b[32m30\u001b[39m\n",
      "\u001b[31mRayTaskError(TypeError)\u001b[39m: \u001b[36mray::TrainWorkerActor.train_epoch()\u001b[39m (pid=273, ip=172.19.0.4, actor_id=012b9ff22cdea402925c322f01000000, repr=<__main__.TrainWorkerActor object at 0x7dae17b67b90>)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_20/1999435513.py\", line 92, in train_epoch\nTypeError: not all arguments converted during string formatting"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# ============================================================\n",
    "# 0. CONFIG\n",
    "# ============================================================\n",
    "# user side\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    delta_path: str             # Delta table path\n",
    "    epochs: int\n",
    "    batch_size: int\n",
    "    num_workers: int\n",
    "    shuffle_each_epoch: bool = True\n",
    "    user_params: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. BASE TRAIN WORKER (USER IMPLEMENTS THIS)\n",
    "# ============================================================\n",
    "\n",
    "# user side\n",
    "class BaseTrainWorker(ABC):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def init_model(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def initialize(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_step(self, model, batch, epoch: int, config: TrainConfig):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def post_epoch(self, epoch: int, metrics):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def post_train(self, final_metrics):\n",
    "        pass\n",
    "\n",
    "    # Protected internal methods\n",
    "    def _setup(self):\n",
    "        self.model = self.init_model()\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"init_model() must return a model/state object\")\n",
    "        self.initialize()\n",
    "\n",
    "    def _train_epoch(self, batch_iter, epoch: int):\n",
    "        all_metrics = []\n",
    "        for batch_idx, batch in enumerate(batch_iter):\n",
    "            metrics = self.train_step(self.model, batch, epoch, self.config)\n",
    "            metrics[\"batch\"] = batch_idx\n",
    "            print(f\"[Worker] Epoch={epoch} Batch={batch_idx} Metrics={metrics}\")\n",
    "            all_metrics.append(metrics)\n",
    "        self.post_epoch(epoch, all_metrics)\n",
    "\n",
    "        return all_metrics\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. TRAIN WORKER (RAY ACTOR)\n",
    "# ============================================================\n",
    "\n",
    "@ray.remote\n",
    "class TrainWorkerActor:\n",
    "    def __init__(self, trainer_cls, config: TrainConfig):\n",
    "        self.config = config\n",
    "        self.trainer = trainer_cls(config)\n",
    "\n",
    "        # Lazy dataset load (inside actor!)\n",
    "        self.dataset = None\n",
    "\n",
    "    def setup(self):\n",
    "        print(\"[Worker] Running setup...\")\n",
    "        self.dataset = ray.data.read_delta(self.config.delta_path)\n",
    "        self.trainer._setup()\n",
    "\n",
    "    def train_epoch(self, epoch: int):\n",
    "        # shuffle per epoch\n",
    "        ds_epoch = self.dataset.random_shuffle() if self.config.shuffle_each_epoch else self.dataset\n",
    "        # shard for this worker (dataset split by total workers)\n",
    "        shards = ds_epoch.split(self.config.num_workers)\n",
    "        worker_idx = ray.get_runtime_context().get_worker_id() % self.config.num_workers\n",
    "        shard = shards[worker_idx]\n",
    "\n",
    "        batch_iter = shard.iter_batches(batch_size=self.config.batch_size)\n",
    "        return self.trainer._train_epoch(batch_iter, epoch)\n",
    "\n",
    "    def finalize(self, final_metrics):\n",
    "        return self.trainer.post_train(final_metrics)\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.trainer.model\n",
    "# user side\n",
    "def train(trainer_cls, config: TrainConfig):\n",
    "    trainer = BaseTrainer(config=config, trainer_cls=trainer_cls)\n",
    "    return trainer.train()\n",
    "\n",
    "# ============================================================\n",
    "# 3. DRIVER TRAINER\n",
    "# ============================================================\n",
    "\n",
    "class BaseTrainer:\n",
    "    def __init__(self, config: TrainConfig, trainer_cls):\n",
    "        self.config = config\n",
    "        self.trainer_cls = trainer_cls\n",
    "\n",
    "    def train(self):\n",
    "        # create actors\n",
    "        workers = [TrainWorkerActor.remote(self.trainer_cls, self.config)\n",
    "                   for _ in range(self.config.num_workers)]\n",
    "\n",
    "        # setup (dataset + model)\n",
    "        ray.get([w.setup.remote() for w in workers])\n",
    "\n",
    "        all_epochs_results = []\n",
    "\n",
    "        for epoch in range(self.config.epochs):\n",
    "            print(f\"[Driver] Starting epoch {epoch}\")\n",
    "            futs = [w.train_epoch.remote(epoch) for w in workers]\n",
    "            epoch_results = ray.get(futs)\n",
    "            all_epochs_results.append(epoch_results)\n",
    "\n",
    "        # finalize\n",
    "        for w in workers:\n",
    "            w.finalize.remote(all_epochs_results)\n",
    "\n",
    "        # collect models\n",
    "        models = ray.get([w.get_model.remote() for w in workers])\n",
    "        return {\"metrics\": all_epochs_results, \"models\": models}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. USER SIDE SDK\n",
    "# ============================================================\n",
    "\n",
    "# sdk.BaseTrainWorker\n",
    "#sdk.train\n",
    "\n",
    "# ============================================================\n",
    "# 5. USER TRAINER EXAMPLE\n",
    "# ============================================================\n",
    "\n",
    "class MyTrainer(BaseTrainWorker):\n",
    "    def init_model(self):\n",
    "        return {\"w\": 0.0}\n",
    "\n",
    "    def initialize(self):\n",
    "        print(\"[User] Init step\")\n",
    "\n",
    "    def train_step(self, model, batch, epoch, config):\n",
    "        x = batch[\"x\"]\n",
    "        model[\"w\"] += sum(x) * 0.01\n",
    "        return {\"update\": float(model[\"w\"])}\n",
    "\n",
    "    def post_epoch(self, epoch, metrics):\n",
    "        print(f\"[User] Post epoch {epoch}, metrics_count={len(metrics)}\")\n",
    "\n",
    "    def post_train(self, final_metrics):\n",
    "        print(\"[User] Training complete.\")\n",
    "\n",
    "\n",
    "config = TrainConfig(\n",
    "    delta_path=\"s3://mybucket/data-delta/\",\n",
    "    epochs=2,\n",
    "    batch_size=50,\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "result =  train(MyTrainer, config)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "369e6b11-d9c2-4ea2-b805-e7dca8f4d626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PredictWorkerActor pid=719)\u001b[0m [Worker] Setup prediction...\n",
      "\u001b[36m(PredictWorkerActor pid=1305)\u001b[0m [Worker] Setup prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PredictWorkerActor pid=719)\u001b[0m Estimated parquet encoding ratio is 6.463.\n",
      "\u001b[36m(PredictWorkerActor pid=719)\u001b[0m Estimated parquet reader batch size at 163881 rows\n",
      "\u001b[36m(PredictWorkerActor pid=1305)\u001b[0m Estimated parquet encoding ratio is 6.463.\n",
      "\u001b[36m(PredictWorkerActor pid=1305)\u001b[0m Estimated parquet reader batch size at 163881 rows\n",
      "\u001b[36m(PredictWorkerActor pid=719)\u001b[0m A new progress UI is available. To enable, set `ray.data.DataContext.get_current().enable_rich_progress_bars = True`.\n",
      "\u001b[36m(PredictWorkerActor pid=719)\u001b[0m Registered dataset logger for dataset dataset_6_0\n",
      "\u001b[36m(PredictWorkerActor pid=719)\u001b[0m Starting execution of Dataset dataset_6_0. Full logs are in /tmp/ray/session_2025-11-17_00-59-52_408689_7/logs/ray-data\n",
      "\u001b[36m(PredictWorkerActor pid=719)\u001b[0m Execution plan of Dataset dataset_6_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet]\n",
      "\u001b[36m(PredictWorkerActor pid=719)\u001b[0m ⚠️  Ray's object store is configured to use only 42.9% of available memory (11.2GiB out of 26.1GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
      "\u001b[36m(PredictWorkerActor pid=1305)\u001b[0m A new progress UI is available. To enable, set `ray.data.DataContext.get_current().enable_rich_progress_bars = True`.\n",
      "\u001b[36m(PredictWorkerActor pid=1305)\u001b[0m Registered dataset logger for dataset dataset_7_0\n",
      "\u001b[36m(PredictWorkerActor pid=1305)\u001b[0m Starting execution of Dataset dataset_7_0. Full logs are in /tmp/ray/session_2025-11-17_00-59-52_408689_7/logs/ray-data\n",
      "\u001b[36m(PredictWorkerActor pid=1305)\u001b[0m Execution plan of Dataset dataset_7_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet]\n",
      "\u001b[36m(PredictWorkerActor pid=1305)\u001b[0m ⚠️  Ray's object store is configured to use only 42.9% of available memory (11.2GiB out of 26.1GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
      "\u001b[36m(PredictWorkerActor pid=719)\u001b[0m ✔️  Dataset dataset_6_0 execution finished in 21.86 seconds\n",
      "\u001b[36m(PredictWorkerActor pid=719)\u001b[0m Registered dataset logger for dataset dataset_8_0\n"
     ]
    },
    {
     "ename": "RayTaskError(TypeError)",
     "evalue": "\u001b[36mray::PredictWorkerActor.predict()\u001b[39m (pid=719, ip=172.19.0.2, actor_id=f29a2655c1b91131484a5d0801000000, repr=<__main__.PredictWorkerActor object at 0x7b37e1a1a990>)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_20/1517637419.py\", line 73, in predict\n  File \"/tmp/ipykernel_20/1517637419.py\", line 45, in _predict_epoch\n  File \"/tmp/ipykernel_20/1517637419.py\", line 137, in predict_batch\nTypeError: unsupported operand type(s) for *: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRayTaskError(TypeError)\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 150\u001b[39m\n\u001b[32m    141\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[User] Prediction complete.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    144\u001b[39m config = PredictionConfig(\n\u001b[32m    145\u001b[39m     delta_path=\u001b[33m\"\u001b[39m\u001b[33ms3://mybucket/data-delta/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    146\u001b[39m     batch_size=\u001b[32m64\u001b[39m,\n\u001b[32m    147\u001b[39m     num_workers=\u001b[32m2\u001b[39m,\n\u001b[32m    148\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m results = \u001b[43mbatch_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMyPredictor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 124\u001b[39m, in \u001b[36mbatch_predict\u001b[39m\u001b[34m(config, worker_cls, predict_fn)\u001b[39m\n\u001b[32m    121\u001b[39m     worker_cls = _WrappedWorker\n\u001b[32m    123\u001b[39m predictor = BasePredictor(config, worker_cls)\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36mBasePredictor.predict\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# parallel prediction\u001b[39;00m\n\u001b[32m     97\u001b[39m futures = [\n\u001b[32m     98\u001b[39m     workers[i].predict.remote(i)\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.config.num_workers)\n\u001b[32m    100\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m results = \u001b[43mray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m workers:\n\u001b[32m    105\u001b[39m     w.finalize.remote(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:22\u001b[39m, in \u001b[36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mauto_init_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     21\u001b[39m     auto_init_ray()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/ray/_private/client_mode_hook.py:103\u001b[39m, in \u001b[36mclient_mode_hook.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m client_mode_should_convert():\n\u001b[32m    100\u001b[39m     \u001b[38;5;66;03m# Legacy code\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# we only convert init function if RAY_CLIENT_MODE=1\u001b[39;00m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func.\u001b[34m__name__\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33minit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/ray/util/client/api.py:42\u001b[39m, in \u001b[36m_ClientAPI.get\u001b[39m\u001b[34m(self, vals, timeout)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, vals, *, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     36\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"get is the hook stub passed on to replace `ray.get`\u001b[39;00m\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03m        vals: [Client]ObjectRef or list of these refs to retrieve.\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m        timeout: Optional timeout in milliseconds\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/ray/util/client/worker.py:433\u001b[39m, in \u001b[36mWorker.get\u001b[39m\u001b[34m(self, vals, timeout)\u001b[39m\n\u001b[32m    431\u001b[39m     op_timeout = max_blocking_operation_time\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_get\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GetTimeoutError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/ray/util/client/worker.py:461\u001b[39m, in \u001b[36mWorker._get\u001b[39m\u001b[34m(self, ref, timeout)\u001b[39m\n\u001b[32m    459\u001b[39m         logger.exception(\u001b[33m\"\u001b[39m\u001b[33mFailed to deserialize \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(chunk.error))\n\u001b[32m    460\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunk.total_size > OBJECT_TRANSFER_WARNING_SIZE \u001b[38;5;129;01mand\u001b[39;00m log_once(\n\u001b[32m    463\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mclient_object_transfer_size_warning\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    464\u001b[39m ):\n\u001b[32m    465\u001b[39m     size_gb = chunk.total_size / \u001b[32m2\u001b[39m**\u001b[32m30\u001b[39m\n",
      "\u001b[31mRayTaskError(TypeError)\u001b[39m: \u001b[36mray::PredictWorkerActor.predict()\u001b[39m (pid=719, ip=172.19.0.2, actor_id=f29a2655c1b91131484a5d0801000000, repr=<__main__.PredictWorkerActor object at 0x7b37e1a1a990>)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_20/1517637419.py\", line 73, in predict\n  File \"/tmp/ipykernel_20/1517637419.py\", line 45, in _predict_epoch\n  File \"/tmp/ipykernel_20/1517637419.py\", line 137, in predict_batch\nTypeError: unsupported operand type(s) for *: 'NoneType' and 'int'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PredictWorkerActor pid=719)\u001b[0m ✔️  Dataset dataset_8_0 execution finished in 21.86 seconds\n",
      "\u001b[36m(PredictWorkerActor pid=1305)\u001b[0m ✔️  Dataset dataset_7_0 execution finished in 25.23 seconds\n",
      "\u001b[36m(PredictWorkerActor pid=1305)\u001b[0m Registered dataset logger for dataset dataset_11_0\n",
      "\u001b[36m(PredictWorkerActor pid=1305)\u001b[0m ✔️  Dataset dataset_11_0 execution finished in 25.23 seconds\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, Callable, Iterable\n",
    "import ray\n",
    "\n",
    "@dataclass\n",
    "class PredictionConfig:\n",
    "    delta_path: str              # or dataset passed separately\n",
    "    batch_size: int\n",
    "    num_workers: int\n",
    "    shuffle: bool = False\n",
    "    user_params: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "\n",
    "class BasePredictWorker:\n",
    "    \"\"\"\n",
    "    User must subclass and implement:\n",
    "        - init_model()\n",
    "        - initialize()\n",
    "        - predict_batch(model, batch, config)\n",
    "        - post_predict(all_outputs)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: PredictionConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "\n",
    "    # ---------- USER MUST IMPLEMENT ----------\n",
    "    def init_model(self): raise NotImplementedError\n",
    "    def initialize(self): pass\n",
    "    def predict_batch(self, model, batch, config): raise NotImplementedError\n",
    "    def post_predict(self, all_outputs): pass\n",
    "    # -----------------------------------------\n",
    "\n",
    "    def _setup(self):\n",
    "        self.model = self.init_model()\n",
    "        self.initialize()\n",
    "\n",
    "    def _predict_epoch(self, shard, config: PredictionConfig):\n",
    "        \"\"\"Iterate over shard batches and call predict_batch.\"\"\"\n",
    "        batch_iter = shard.iter_batches(batch_size=config.batch_size)\n",
    "\n",
    "        outputs = []\n",
    "        for batch_idx, batch in enumerate(batch_iter):\n",
    "            pred = self.predict_batch(self.model, batch, config)\n",
    "            print(f\"[Worker] Batch={batch_idx} Pred={str(pred)[:200]}\")\n",
    "            outputs.append(pred)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class PredictWorkerActor:\n",
    "    def __init__(self, worker_cls, config: PredictionConfig):\n",
    "        self.worker = worker_cls(config)\n",
    "        self.config = config\n",
    "        self.dataset = None\n",
    "\n",
    "    def setup(self):\n",
    "        print(\"[Worker] Setup prediction...\")\n",
    "        # dataset loaded inside worker\n",
    "        self.dataset = ray.data.read_delta(self.config.delta_path)\n",
    "        self.worker._setup()\n",
    "\n",
    "    def predict(self, worker_index: int):\n",
    "        ds = self.dataset\n",
    "        if self.config.shuffle:\n",
    "            ds = ds.random_shuffle()\n",
    "\n",
    "        shards = ds.split(self.config.num_workers)\n",
    "\n",
    "        shard = shards[worker_index]\n",
    "        return self.worker._predict_epoch(shard, self.config)\n",
    "\n",
    "    def finalize(self, all_outputs):\n",
    "        return self.worker.post_predict(all_outputs)\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.worker.model\n",
    "\n",
    "\n",
    "class BasePredictor:\n",
    "    def __init__(self, config: PredictionConfig, worker_cls):\n",
    "        self.config = config\n",
    "        self.worker_cls = worker_cls\n",
    "\n",
    "    def predict(self):\n",
    "        # spawn workers\n",
    "        workers = [\n",
    "            PredictWorkerActor.remote(self.worker_cls, self.config)\n",
    "            for _ in range(self.config.num_workers)\n",
    "        ]\n",
    "\n",
    "        ray.get([w.setup.remote() for w in workers])\n",
    "\n",
    "        # parallel prediction\n",
    "        futures = [\n",
    "            workers[i].predict.remote(i)\n",
    "            for i in range(self.config.num_workers)\n",
    "        ]\n",
    "\n",
    "        results = ray.get(futures)\n",
    "\n",
    "        for w in workers:\n",
    "            w.finalize.remote(results)\n",
    "\n",
    "        models = ray.get([w.get_model.remote() for w in workers])\n",
    "\n",
    "        return {\"results\": results, \"models\": models}\n",
    "\n",
    "\n",
    "def batch_predict(config: PredictionConfig, worker_cls=None, predict_fn=None):\n",
    "    if worker_cls is None:\n",
    "        if predict_fn is None:\n",
    "            raise ValueError(\"Provide either worker_cls or predict_fn.\")\n",
    "\n",
    "        class _WrappedWorker(BasePredictWorker):\n",
    "            def init_model(self): return None\n",
    "            def predict_batch(self, model, batch, config): return predict_fn(batch)\n",
    "\n",
    "        worker_cls = _WrappedWorker\n",
    "\n",
    "    predictor = BasePredictor(config, worker_cls)\n",
    "    return predictor.predict()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### TEST\n",
    "\n",
    "class MyPredictor(BasePredictWorker):\n",
    "    def init_model(self):\n",
    "        return {\"multiplier\": 2}\n",
    "\n",
    "    def predict_batch(self, model, batch, config):\n",
    "        # simple example\n",
    "        return {k: [v_i * model[\"multiplier\"] for v_i in batch[k]]\n",
    "                for k in batch}\n",
    "\n",
    "    def post_predict(self, all_outputs):\n",
    "        print(\"[User] Prediction complete.\")\n",
    "\n",
    "\n",
    "config = PredictionConfig(\n",
    "    delta_path=\"s3://mybucket/data-delta/\",\n",
    "    batch_size=64,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "results = batch_predict(config, worker_cls=MyPredictor)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca3747ab-db15-482c-8847-b719ef171162",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T15:35:20.505043Z",
     "iopub.status.busy": "2025-11-16T15:35:20.504689Z",
     "iopub.status.idle": "2025-11-16T15:35:48.462021Z",
     "shell.execute_reply": "2025-11-16T15:35:48.460853Z",
     "shell.execute_reply.started": "2025-11-16T15:35:20.505014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading batch of 2 from s3://mybucket/data-delta/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(explorative_take_batch pid=739)\u001b[0m Estimated parquet encoding ratio is 6.463.\n",
      "\u001b[36m(explorative_take_batch pid=739)\u001b[0m Estimated parquet reader batch size at 163881 rows\n",
      "\u001b[36m(explorative_take_batch pid=739)\u001b[0m Registered dataset logger for dataset dataset_5_0\n",
      "\u001b[36m(explorative_take_batch pid=739)\u001b[0m Starting execution of Dataset dataset_5_0. Full logs are in /tmp/ray/session_2025-11-16_07-26-18_082084_7/logs/ray-data\n",
      "\u001b[36m(explorative_take_batch pid=739)\u001b[0m Execution plan of Dataset dataset_5_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> LimitOperator[limit=2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Read complete.\n",
      "pyarrow.Table\n",
      "Table_id: int32\n",
      "Session_ID: int64\n",
      "Interface_ID: int32\n",
      "Session_Indicator: int32\n",
      "Start_Time_s: int64\n",
      "Start_Time_ms: int32\n",
      "End_Time_s: int64\n",
      "End_Time_ms: int32\n",
      "Protocol_Category: int32\n",
      "Protocol: int32\n",
      "Layer_7_Bearer_Protocol: int32\n",
      "MSISDN: int32\n",
      "IMSI: int32\n",
      "IMEI: int32\n",
      "Roaming_Type: int32\n",
      "Roaming_Direction: int32\n",
      "MS_IP_Address: string\n",
      "Server_IP_Address: string\n",
      "MS_Port: int32\n",
      "Server_Port: int32\n",
      "APN: string\n",
      "SGSN_Signaling_Plane_IP_Address: string\n",
      "GGSN_Signaling_Plane_IP_Address: string\n",
      "SGSN_User_Plane_IP_Address: string\n",
      "GGSN_User_Plane_IP_Address: string\n",
      "MCC: string\n",
      "MNC: string\n",
      "RAT: int32\n",
      "LAC: string\n",
      "RAC: string\n",
      "SAC: string\n",
      "CI: string\n",
      "Browser_Type: int32\n",
      "Uplink_Traffic: int64\n",
      "Downlink_Traffic: int64\n",
      "Network_Uplink_Traffic: int64\n",
      "Network_Downlink_Traffic: int64\n",
      "Uplink_Packets: int32\n",
      "Downlink_Packets: int32\n",
      "TCP_Connection_Status: int32\n",
      "TCP_Connection_Release_Status: int32\n",
      "TCP_Connect_RTT_ms: int64\n",
      "TCP_Uplink_Out_of_Order_Packets: int32\n",
      "TCP_Downlink_Out_of_Order_Packets: int32\n",
      "TCP_Uplink_Retransmitted_Packets: int32\n",
      "TCP_Downlink_Retransmitted_Packets: int32\n",
      "SYN_ACK_Failures_During_TCP_Handshakes: int32\n",
      "ACK_Failures_During_TCP_Handshakes: int32\n",
      "Host_Name: string\n",
      "First_URI_Visited: string\n",
      "Transaction_Type: int32\n",
      "First_GET_Success_or_Failure_Flag: int32\n",
      "First_GET_Transaction_Failure_Cause_Code: int32\n",
      "First_GET_Response_Delay: double\n",
      "GET_Transactions: int32\n",
      "Successful_GET_Transactions: int32\n",
      "POST_Transactions: int32\n",
      "Successful_POST_Transactions: int32\n",
      "Page_Browsing_Delay: double\n",
      "TAC: string\n",
      "ECI: string\n",
      "TCP_RTT_of_Step_1_ms: int32\n",
      "TCP_Uplink_Retransmitted_Packets_with_Payload: int32\n",
      "TCP_Downlink_Retransmitted_Packets_with_Payload: int32\n",
      "TCP_Uplink_Packets_with_Payload: int32\n",
      "TCP_Downlink_Packets_with_Payload: int32\n",
      "RAN_NE_User_Plane_IP_Address: string\n",
      "First_DNS_Response_Delay_Within_Page_ms: double\n",
      "First_DNS_Response_Code_Within_Page: int32\n",
      "Page_Size: int32\n",
      "First_Page_Response_Delay_ms: double\n",
      "MCC_of_Home_Carrier: string\n",
      "MNC_of_Home_Carrier: string\n",
      "User_Agent: string\n",
      "Uplink_Valid_Data_Transmission_Duration: double\n",
      "Downlink_Valid_Data_Transmission_Duration: double\n",
      "Device_Window_Size_Calculations: int32\n",
      "Device_Side_Small_Window_Times: int32\n",
      "Delay_Between_First_TCP_and_First_GET_ms: int32\n",
      "Delay_Between_First_GET_ACK_and_First_Data_Packet_ms: int32\n",
      "Average_Uplink_RTT: int32\n",
      "Average_Downlink_RTT: int32\n",
      "Number_of_Timer_Average_Uplink_RTT_Is_Longer_Than_500_ms: int32\n",
      "Number_of_Timer_Average_Downlink_RTT_Is_Longer_Than_1s: int32\n",
      "Uplink_RTT_Calculations: int32\n",
      "Downlink_RTT_Calculations: int32\n",
      "User_Side_Uplink_Lost_Packets: int32\n",
      "Server_Side_Uplink_Lost_Packets: int32\n",
      "Server_Side_Downlink_Lost_Packets: int32\n",
      "User_Side_Downlink_Lost_Packets: int32\n",
      "Downlink_TCP_Send_Window_Average_Size: int32\n",
      "Total_Downlink_TCP_Send_Window_Size_Calculations: int32\n",
      "Downlink_Maximum_TCP_Send_Window_Size: int32\n",
      "Total_Uplink_TCP_Send_Window_Average_Size: int32\n",
      "Total_Uplink_TCP_Send_Window_Size_Calculations: int32\n",
      "Uplink_Maximum_TCP_Send_Window_Size: int32\n",
      "Downlink_Continuous_Retransmission_Delay: int64\n",
      "User_Side_Hungry_Delay: int64\n",
      "Server_Side_Hunger_Delay: int64\n",
      "Signaling_Consumption_Flag_Signaling_Messages: int32\n",
      "First_RAT: int32\n",
      "Probe_Protocol: int32\n",
      "Visit_Type: int32\n",
      "Uplink_Valid_Traffic: int64\n",
      "Time_Zone: int32\n",
      "DST_Offset: int32\n",
      "Negotiated_Allocation_Retention_Priority: int32\n",
      "Allowed_Uplink_Maximum_Bit_Rate: int32\n",
      "Allowed_Downlink_Maximum_Bit_Rate: int32\n",
      "Negotiated_Traffic_Class: int32\n",
      "Negotiated_Handling_Traffic_Priority: int32\n",
      "Negotiated_QoS_Class_Identifier: int32\n",
      "Total_TCP_Connect_RTT_ms: int64\n",
      "Total_TCP_RTT_of_Step_1_ms: int64\n",
      "APP_ID: string\n",
      "probe_ID: string\n",
      "Record_type: string\n",
      "TCP_Uplink_Payload_Traffic: string\n",
      "TCP_Downlink_Payload_Traffic: string\n",
      "PAGE_SUCCEED_FLAG: int32\n",
      "PAGE_SR_SUCCEED_FLAG: string\n",
      "SP: string\n",
      "FIRSTDNSSUCFLAG: int32\n",
      "UFDR_Type: string\n",
      "DL_TRANS_DELAY: string\n",
      "UL_TRANS_DELAY: string\n",
      "AVG_UL_RTT_MICRO_SEC: string\n",
      "AVG_DW_RTT_MICRO_SEC: string\n",
      "ERROR_CODE_4xx_TIMES: int64\n",
      "ERROR_CODE_5xx_TIMES: int64\n",
      "GET_TIMEOUT_NUM: int64\n",
      "POST_TO_NUM: int64\n",
      "Flag: int32\n",
      "RECORD_DATE_TEHRAN: timestamp[ns]\n",
      "----\n",
      "Table_id: [[3101,3101]]\n",
      "Session_ID: [[1305130618271289494,1305130623311911163]]\n",
      "Interface_ID: [[1,1]]\n",
      "Session_Indicator: [[2,1]]\n",
      "Start_Time_s: [[1754927850,1754928354]]\n",
      "Start_Time_ms: [[64,399]]\n",
      "End_Time_s: [[1754927851,1754928355]]\n",
      "End_Time_ms: [[67,769]]\n",
      "Protocol_Category: [[4,4]]\n",
      "Protocol: [[48000,48000]]\n",
      "...\n",
      "Initiating writer for s3://mybucket/data-delta/ in 'append' mode...\n",
      "...Writer initiated.\n",
      "Writing batch back to s3://mybucket/data-delta/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(explorative_take_batch pid=739)\u001b[0m ✔️  Dataset dataset_5_0 execution finished in 12.28 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Write complete!\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "delta_path = \"s3://mybucket/data-delta/\"\n",
    "\n",
    "print(f\"Reading batch of 2 from {delta_path}...\")\n",
    "batch_to_write = sdk.explorative_take_batch(delta_path, 2)\n",
    "print(\"...Read complete.\")\n",
    "print(batch_to_write)\n",
    "\n",
    "print(f\"Initiating writer for {delta_path} in 'append' mode...\")\n",
    "writer_actor = sdk.initiate_data_writer(delta_path, mode=\"append\")\n",
    "print(\"...Writer initiated.\")\n",
    "\n",
    "print(f\"Writing batch back to {delta_path}...\")\n",
    "write_future = sdk.write_data(writer_actor, batch_to_write)\n",
    "ray.get(write_future)  # <- now valid\n",
    "print(\"...Write complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2e940b-80d3-4986-91bb-3ac23a4eb76d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "021a6252-fe3d-4ecd-a680-258d2398660a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T15:35:17.174535Z",
     "iopub.status.busy": "2025-11-16T15:35:17.173856Z",
     "iopub.status.idle": "2025-11-16T15:35:17.464738Z",
     "shell.execute_reply": "2025-11-16T15:35:17.462607Z",
     "shell.execute_reply.started": "2025-11-16T15:35:17.174488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ProtocolVersions(min_reader_version=1, min_writer_version=2, writer_features=None, reader_features=None)\n",
      "After: ProtocolVersions(min_reader_version=3, min_writer_version=7, writer_features=['timestampNtz'], reader_features=['timestampNtz'])\n"
     ]
    }
   ],
   "source": [
    "from deltalake import DeltaTable\n",
    "from deltalake.table import TableFeatures\n",
    "\n",
    "delta_path = \"s3://mybucket/data-delta/\"\n",
    "\n",
    "dt = DeltaTable(delta_path)\n",
    "\n",
    "# Optional: see current protocol\n",
    "print(\"Before:\", dt.protocol())\n",
    "\n",
    "dt.alter.add_feature(\n",
    "    TableFeatures.TimestampWithoutTimezone,\n",
    "    allow_protocol_versions_increase=True,\n",
    ")\n",
    "\n",
    "print(\"After:\", dt.protocol())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43577f84-62ea-4f4f-b0c8-814ab8f7a168",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T15:36:27.137601Z",
     "iopub.status.busy": "2025-11-16T15:36:27.137128Z",
     "iopub.status.idle": "2025-11-16T15:36:41.705003Z",
     "shell.execute_reply": "2025-11-16T15:36:41.701729Z",
     "shell.execute_reply.started": "2025-11-16T15:36:27.137566Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_explorative_take_batch pid=740)\u001b[0m Estimated parquet encoding ratio is 6.463.\n",
      "\u001b[36m(_explorative_take_batch pid=740)\u001b[0m Estimated parquet reader batch size at 163881 rows\n",
      "\u001b[36m(_explorative_take_batch pid=740)\u001b[0m A new progress UI is available. To enable, set `ray.data.DataContext.get_current().enable_rich_progress_bars = True`.\n",
      "\u001b[36m(_explorative_take_batch pid=740)\u001b[0m Registered dataset logger for dataset dataset_7_0\n",
      "\u001b[36m(_explorative_take_batch pid=740)\u001b[0m Starting execution of Dataset dataset_7_0. Full logs are in /tmp/ray/session_2025-11-16_07-26-18_082084_7/logs/ray-data\n",
      "\u001b[36m(_explorative_take_batch pid=740)\u001b[0m Execution plan of Dataset dataset_7_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> LimitOperator[limit=2]\n",
      "\u001b[36m(_explorative_take_batch pid=740)\u001b[0m ⚠️  Ray's object store is configured to use only 42.9% of available memory (11.2GiB out of 26.1GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
      "\u001b[36m(_explorative_take_batch pid=740)\u001b[0m ✔️  Dataset dataset_7_0 execution finished in 8.42 seconds\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from typing import Callable, Dict, Iterator, Union\n",
    "import pyarrow\n",
    "import pandas\n",
    "import numpy\n",
    "ctx = ray.data.DataContext.get_current()\n",
    "ctx.execution_options.preserve_order = True\n",
    "import deltalake\n",
    "RANDOM_SEED = 42\n",
    "DATA_DEFAULT_BATCH_SIZE = 64\n",
    "\n",
    "# Define the packages your Ray workers need\n",
    "# runtime_env = {\n",
    "#     \"pip\": [\n",
    "#         \"deltalake\",\n",
    "#         \"pyarrow[s3]\",\n",
    "#         \"boto3\",\n",
    "#         \"ray[data]\"\n",
    "#     ]\n",
    "# }\n",
    "# ray.init(address=\"ray://ray-head:10001\", runtime_env=runtime_env)\n",
    "\n",
    "# def read_delta(delta_path:str):\n",
    "#     ds = ray.data.read_delta(delta_path)\n",
    "#     return ds\n",
    "    \n",
    "# def preprocess_ds(ds:ray.data.Dataset, fn: Callable[[pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]], pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]] | Callable[[pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]], Iterator[pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]]]):\n",
    "#     ds = ds.map_batches(fn)\n",
    "#     return ds\n",
    "\n",
    "\n",
    "# def read_data_internal(delta_path:str, preprocess_func:Callable[[pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]], pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]] | Callable[[pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]], Iterator[pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]]]=None):\n",
    "#     ds = read_delta(delta_path)\n",
    "\n",
    "#     if preprocess_func:\n",
    "#         ds = preprocess_ds(ds, preprocess_func)\n",
    "\n",
    "#     return ds\n",
    "\n",
    "# ## user facing function\n",
    "# def _read_data(delta_path:str, preprocess_func:Callable[[pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]], pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]] | Callable[[pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]], Iterator[pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]]]=None):\n",
    "#     return read_data_internal(delta_path, preprocess_func)\n",
    "\n",
    "# ## user facing function\n",
    "# @ray.remote\n",
    "# def _explorative_take_batch(ds:ray.data.Dataset, batch_size: int = DATA_DEFAULT_BATCH_SIZE, *, batch_format: str | None = 'default'):\n",
    "#     return ds.take_batch(batch_size=batch_size, batch_format=batch_format)\n",
    "\n",
    "@ray.remote\n",
    "def _explorative_take_batch(delta_path:str, batch_size: int = DATA_DEFAULT_BATCH_SIZE, preprocess_func:Callable[[pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]], pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]] | Callable[[pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]], Iterator[pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]]]=None, batch_format: str | None = 'pyarrow'):\n",
    "    ds = ray.data.read_delta(delta_path)\n",
    "    if preprocess_func:\n",
    "        ds = ds.map_batches(preprocess_func)\n",
    "    return ds.take_batch(batch_size=batch_size, batch_format=batch_format)\n",
    "\n",
    "## user facing function\n",
    "def explorative_take_batch(delta_path:str, batch_size: int = DATA_DEFAULT_BATCH_SIZE, preprocess_func:Callable[[pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]], pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]] | Callable[[pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]], Iterator[pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]]]=None, batch_format: str | None = 'pyarrow'):\n",
    "    return ray.get(_explorative_take_batch.remote(delta_path=delta_path, batch_size=batch_size, preprocess_func=preprocess_func, batch_format=batch_format))\n",
    "\n",
    "\n",
    "# @ray.remote\n",
    "# def _explorative_take_batches(delta_path:str, batch_size: int = DATA_DEFAULT_BATCH_SIZE, num_batches:int = 1, preprocess_func:Callable[[pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]], pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]] | Callable[[pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]], Iterator[pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]]]=None, batch_format: str | None = 'default'):\n",
    "#     ds = ray.data.read_delta(delta_path)\n",
    "#     if preprocess_func:\n",
    "#         ds = ds.map_batches(preprocess_func)\n",
    "#     # FIX\n",
    "#     return ds.take_batch(batch_size=batch_size, batch_format=batch_format)\n",
    "\n",
    "# ## user facing function\n",
    "# def explorative_take_batches(delta_path:str, batch_size: int = DATA_DEFAULT_BATCH_SIZE, num_batches:int = 1, preprocess_func:Callable[[pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]], pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]] | Callable[[pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]], Iterator[pyarrow.Table | pandas.DataFrame | Dict[str, numpy.ndarray]]]=None, batch_format: str | None = 'default'):\n",
    "#     return ray.get(_explorative_take_batches.remote(delta_path=delta_path, batch_size=batch_size, num_batches=num_batches, preprocess_func=preprocess_func, batch_format=batch_format))\n",
    "\n",
    "# FIX\n",
    "# @ray.remote\n",
    "# def _run_per_batch()\n",
    "\n",
    "\n",
    "## user facing function\n",
    "# def _explorative_iter_batches(ds:ray.data.Dataset, batch_size: int = DATA_DEFAULT_BATCH_SIZE, *, batch_format: str | None = 'default'):\n",
    "#     return ds.iter_batches(batch_size=batch_size, batch_format=batch_format, local_shuffle_seed=RANDOM_SEED)\n",
    "# print(_read_data(\"s3://mybucket/data-delta/\"))\n",
    "\n",
    "batch = explorative_take_batch(\"s3://mybucket/data-delta/\", 2, batch_format = \"pyarrow\")\n",
    "#print(ray.get(_explorative_take_batch.remote(\"s3://mybucket/data-delta/\", 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cfc1846-9e8b-4462-b05c-af9fd699864a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T15:36:49.817595Z",
     "iopub.status.busy": "2025-11-16T15:36:49.816803Z",
     "iopub.status.idle": "2025-11-16T15:36:49.830035Z",
     "shell.execute_reply": "2025-11-16T15:36:49.828295Z",
     "shell.execute_reply.started": "2025-11-16T15:36:49.817554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "Table_id: int32\n",
       "Session_ID: int64\n",
       "Interface_ID: int32\n",
       "Session_Indicator: int32\n",
       "Start_Time_s: int64\n",
       "Start_Time_ms: int32\n",
       "End_Time_s: int64\n",
       "End_Time_ms: int32\n",
       "Protocol_Category: int32\n",
       "Protocol: int32\n",
       "Layer_7_Bearer_Protocol: int32\n",
       "MSISDN: int32\n",
       "IMSI: int32\n",
       "IMEI: int32\n",
       "Roaming_Type: int32\n",
       "Roaming_Direction: int32\n",
       "MS_IP_Address: string\n",
       "Server_IP_Address: string\n",
       "MS_Port: int32\n",
       "Server_Port: int32\n",
       "APN: string\n",
       "SGSN_Signaling_Plane_IP_Address: string\n",
       "GGSN_Signaling_Plane_IP_Address: string\n",
       "SGSN_User_Plane_IP_Address: string\n",
       "GGSN_User_Plane_IP_Address: string\n",
       "MCC: string\n",
       "MNC: string\n",
       "RAT: int32\n",
       "LAC: string\n",
       "RAC: string\n",
       "SAC: string\n",
       "CI: string\n",
       "Browser_Type: int32\n",
       "Uplink_Traffic: int64\n",
       "Downlink_Traffic: int64\n",
       "Network_Uplink_Traffic: int64\n",
       "Network_Downlink_Traffic: int64\n",
       "Uplink_Packets: int32\n",
       "Downlink_Packets: int32\n",
       "TCP_Connection_Status: int32\n",
       "TCP_Connection_Release_Status: int32\n",
       "TCP_Connect_RTT_ms: int64\n",
       "TCP_Uplink_Out_of_Order_Packets: int32\n",
       "TCP_Downlink_Out_of_Order_Packets: int32\n",
       "TCP_Uplink_Retransmitted_Packets: int32\n",
       "TCP_Downlink_Retransmitted_Packets: int32\n",
       "SYN_ACK_Failures_During_TCP_Handshakes: int32\n",
       "ACK_Failures_During_TCP_Handshakes: int32\n",
       "Host_Name: string\n",
       "First_URI_Visited: string\n",
       "Transaction_Type: int32\n",
       "First_GET_Success_or_Failure_Flag: int32\n",
       "First_GET_Transaction_Failure_Cause_Code: int32\n",
       "First_GET_Response_Delay: double\n",
       "GET_Transactions: int32\n",
       "Successful_GET_Transactions: int32\n",
       "POST_Transactions: int32\n",
       "Successful_POST_Transactions: int32\n",
       "Page_Browsing_Delay: double\n",
       "TAC: string\n",
       "ECI: string\n",
       "TCP_RTT_of_Step_1_ms: int32\n",
       "TCP_Uplink_Retransmitted_Packets_with_Payload: int32\n",
       "TCP_Downlink_Retransmitted_Packets_with_Payload: int32\n",
       "TCP_Uplink_Packets_with_Payload: int32\n",
       "TCP_Downlink_Packets_with_Payload: int32\n",
       "RAN_NE_User_Plane_IP_Address: string\n",
       "First_DNS_Response_Delay_Within_Page_ms: double\n",
       "First_DNS_Response_Code_Within_Page: int32\n",
       "Page_Size: int32\n",
       "First_Page_Response_Delay_ms: double\n",
       "MCC_of_Home_Carrier: string\n",
       "MNC_of_Home_Carrier: string\n",
       "User_Agent: string\n",
       "Uplink_Valid_Data_Transmission_Duration: double\n",
       "Downlink_Valid_Data_Transmission_Duration: double\n",
       "Device_Window_Size_Calculations: int32\n",
       "Device_Side_Small_Window_Times: int32\n",
       "Delay_Between_First_TCP_and_First_GET_ms: int32\n",
       "Delay_Between_First_GET_ACK_and_First_Data_Packet_ms: int32\n",
       "Average_Uplink_RTT: int32\n",
       "Average_Downlink_RTT: int32\n",
       "Number_of_Timer_Average_Uplink_RTT_Is_Longer_Than_500_ms: int32\n",
       "Number_of_Timer_Average_Downlink_RTT_Is_Longer_Than_1s: int32\n",
       "Uplink_RTT_Calculations: int32\n",
       "Downlink_RTT_Calculations: int32\n",
       "User_Side_Uplink_Lost_Packets: int32\n",
       "Server_Side_Uplink_Lost_Packets: int32\n",
       "Server_Side_Downlink_Lost_Packets: int32\n",
       "User_Side_Downlink_Lost_Packets: int32\n",
       "Downlink_TCP_Send_Window_Average_Size: int32\n",
       "Total_Downlink_TCP_Send_Window_Size_Calculations: int32\n",
       "Downlink_Maximum_TCP_Send_Window_Size: int32\n",
       "Total_Uplink_TCP_Send_Window_Average_Size: int32\n",
       "Total_Uplink_TCP_Send_Window_Size_Calculations: int32\n",
       "Uplink_Maximum_TCP_Send_Window_Size: int32\n",
       "Downlink_Continuous_Retransmission_Delay: int64\n",
       "User_Side_Hungry_Delay: int64\n",
       "Server_Side_Hunger_Delay: int64\n",
       "Signaling_Consumption_Flag_Signaling_Messages: int32\n",
       "First_RAT: int32\n",
       "Probe_Protocol: int32\n",
       "Visit_Type: int32\n",
       "Uplink_Valid_Traffic: int64\n",
       "Time_Zone: int32\n",
       "DST_Offset: int32\n",
       "Negotiated_Allocation_Retention_Priority: int32\n",
       "Allowed_Uplink_Maximum_Bit_Rate: int32\n",
       "Allowed_Downlink_Maximum_Bit_Rate: int32\n",
       "Negotiated_Traffic_Class: int32\n",
       "Negotiated_Handling_Traffic_Priority: int32\n",
       "Negotiated_QoS_Class_Identifier: int32\n",
       "Total_TCP_Connect_RTT_ms: int64\n",
       "Total_TCP_RTT_of_Step_1_ms: int64\n",
       "APP_ID: string\n",
       "probe_ID: string\n",
       "Record_type: string\n",
       "TCP_Uplink_Payload_Traffic: string\n",
       "TCP_Downlink_Payload_Traffic: string\n",
       "PAGE_SUCCEED_FLAG: int32\n",
       "PAGE_SR_SUCCEED_FLAG: string\n",
       "SP: string\n",
       "FIRSTDNSSUCFLAG: int32\n",
       "UFDR_Type: string\n",
       "DL_TRANS_DELAY: string\n",
       "UL_TRANS_DELAY: string\n",
       "AVG_UL_RTT_MICRO_SEC: string\n",
       "AVG_DW_RTT_MICRO_SEC: string\n",
       "ERROR_CODE_4xx_TIMES: int64\n",
       "ERROR_CODE_5xx_TIMES: int64\n",
       "GET_TIMEOUT_NUM: int64\n",
       "POST_TO_NUM: int64\n",
       "Flag: int32\n",
       "RECORD_DATE_TEHRAN: timestamp[ns]\n",
       "----\n",
       "Table_id: [[3101,3101]]\n",
       "Session_ID: [[1305130618271289494,1305130623311911163]]\n",
       "Interface_ID: [[1,1]]\n",
       "Session_Indicator: [[2,1]]\n",
       "Start_Time_s: [[1754927850,1754928354]]\n",
       "Start_Time_ms: [[64,399]]\n",
       "End_Time_s: [[1754927851,1754928355]]\n",
       "End_Time_ms: [[67,769]]\n",
       "Protocol_Category: [[4,4]]\n",
       "Protocol: [[48000,48000]]\n",
       "..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecd4b2c8-c90e-4d0b-9a95-a0666302eb44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T15:37:15.451044Z",
     "iopub.status.busy": "2025-11-16T15:37:15.450595Z",
     "iopub.status.idle": "2025-11-16T15:37:15.538984Z",
     "shell.execute_reply": "2025-11-16T15:37:15.536987Z",
     "shell.execute_reply.started": "2025-11-16T15:37:15.451010Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import ray\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from deltalake import write_deltalake  # delta-rs write API\n",
    "import os\n",
    "\n",
    "\n",
    "@ray.remote # should have only one instance per delta path\n",
    "class DeltaWriter: # this should inherit from a more generic writer class later\n",
    "    def __init__(self, delta_path: str, mode: str = \"append\"):\n",
    "        \"\"\"\n",
    "        A SINGLE writer for Delta Lake, ensuring consistency.\n",
    "        \n",
    "        Args:\n",
    "            delta_path: Path to the Delta table (s3://..., gs://..., or local).\n",
    "            mode: 'append' or 'overwrite' for initial write.\n",
    "        \"\"\"\n",
    "        self.delta_path = delta_path\n",
    "        self.mode = mode\n",
    "        # self.initialized = False\n",
    "        \n",
    "    # def _init_table_if_needed(self, df: pa.Table):\n",
    "    #     \"\"\"Create or load Delta table.\"\"\"\n",
    "    #     print(type(df))\n",
    "    #     #df = pa.Table.from_pandas(df)\n",
    "    #     if not self.initialized:\n",
    "    #         if not os.path.exists(self.delta_path):\n",
    "    #             # First write creates the table\n",
    "    #             write_deltalake(\n",
    "    #                 self.delta_path, df, mode=self.mode\n",
    "    #             )\n",
    "    #         else:\n",
    "    #             # Table exists; pass\n",
    "    #             pass\n",
    "    #         self.initialized = True\n",
    "\n",
    "    def write_batch(self, batch: pa.Table) -> str: # TODO: Not only pyarrow table, check what types write_deltalake supports and use them\n",
    "        \"\"\"\n",
    "        Write a single batch to Delta Lake.\n",
    "        This call is serialized because the actor is single-threaded.\n",
    "        \"\"\"\n",
    "        # Each worker calling this will block until previous writes finish\n",
    "        # TODO: have a buffer to keep this from blocking as much as possible\n",
    "        write_deltalake(\n",
    "            self.delta_path,\n",
    "            batch,\n",
    "            mode=\"append\",\n",
    "        )\n",
    "\n",
    "    def flush(self):\n",
    "        \"\"\"Optional: compact or optimize later if needed.\"\"\"\n",
    "        pass\n",
    "\n",
    "# user facing function\n",
    "def initiate_data_writer(delta_path: str, mode: str = \"append\"):\n",
    "    writer = DeltaWriter.remote(delta_path, mode)\n",
    "    return writer\n",
    "\n",
    "# user facing function\n",
    "def write_data(writer: DeltaWriter, batch: pa.Table): ## change batch data type based on the delta writer write batch\n",
    "    writer.write_batch.remote(batch)\n",
    "\n",
    "writer = initiate_data_writer(\"s3://testbucket/\")\n",
    "write_data(writer, batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7494b845-70fc-4c5e-b9fe-56837229136a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
